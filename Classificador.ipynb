{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 2: Carregamento da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment', 'id', 'date', 'query', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/ProjetoIA/training.1600000.processed.noemoticon.csv',\n",
    "                   header = None,\n",
    "                   names = cols,\n",
    "                   engine = 'python',\n",
    "                   encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['id', 'date', 'query', 'user'],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 3: Limpeza dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_tweet(tweet):\n",
    "  tweet = BeautifulSoup(tweet, 'lxml').get_text() #Deixar o texto em um formato adequado para fazer a limpeza\n",
    "  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "  tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "  tweet = re.sub(r\" +\", ' ', tweet)\n",
    "\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = data['text'][0]\n",
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = limpar_tweet(teste)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limpo = [limpar_tweet(tweet) for tweet in data.text] #Utiliza a função para limpar todos os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limpo[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values #Ajustando os valores do sentimento positivo\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 4: Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=False) #Pegar modelo\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() #Adicionando o arquivo de vocabulario\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() #Deixando transformar em minusculo\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence):\n",
    "  return [\"[CLS]\"] + tokenizer.tokenize(sentence) + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entradas = [encode_sentence(sentence) for sentence in data_limpo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 5: Criação da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(tokens):\n",
    "  return tokenizer.convert_tokens_to_ids(tokens) #Converter os tokens para id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(tokens):\n",
    "  return np.char.not_equal(tokens, '[PAD]').astype(int) #Vefica se há um padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(tokens): #Faz a mudança de 0 e 1\n",
    "  seg_ids = []\n",
    "  current_seg_id = 0\n",
    "  for tok in tokens:\n",
    "    seg_ids.append(current_seg_id)\n",
    "    if tok == \"[SEP]\":\n",
    "      current_seg_id = 1 - current_seg_id\n",
    "  return seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sent = [\"[CLS]\"] + tokenizer.tokenize(\"Rose are red\") + ['[SEP]']\n",
    "my_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer([\n",
    "    tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0),\n",
    "    tf.expand_dims(tf.cast(get_mask(my_sent), tf.int32), 0),\n",
    "    tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)] for i, sent in enumerate(data_entradas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_with_len)\n",
    "sorted_all = [([get_ids(sent_lab[0]),\n",
    "                get_mask(sent_lab[0]),\n",
    "                get_segments(sent_lab[0])],\n",
    "                sent_lab[1])\n",
    "              for sent_lab in data_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32)) #Tranformar os dados para que o tensorflow consiga interpretar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((3,None), ()), padding_values=(0, 0)) #Definir o numero de registros em cada batch e criar uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(all_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_BATCHES = len(sorted_all) // BATCH_SIZE #Quantidade de batches para treinamento\n",
    "NB_BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_BATCHES_TEST = NB_BATCHES // 10 #Quantidade de batches para teste\n",
    "NB_BATCHES_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batched.shuffle(NB_BATCHES) #Embaralhar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = all_batched.take(NB_BATCHES_TEST) #Criar base de dados de teste\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST) #Crirar base de dados de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 6: Criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNNBERTEmbedding(tf.keras.Model):\n",
    "\n",
    "  def __init__(self,\n",
    "               nb_filters = 50,\n",
    "               FFN_units=512,\n",
    "               nb_classes = 2,\n",
    "               dropout_rate=0.1,\n",
    "               name=\"dcnn\"):\n",
    "    super(DCNNBERTEmbedding, self).__init__(name=name)\n",
    "\n",
    "    self.bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "                                     trainable=False)\n",
    "\n",
    "    #Definição dos filtros\n",
    "    self.bigram = layers.Conv1D(filters = nb_filters,\n",
    "                                kernel_size = 2,\n",
    "                                padding = 'valid',\n",
    "                                activation = 'relu')\n",
    "\n",
    "    self.trigram = layers.Conv1D(filters = nb_filters,\n",
    "                                kernel_size = 3,\n",
    "                                padding = 'valid',\n",
    "                                activation = 'relu')\n",
    "    self.fourgram = layers.Conv1D(filters = nb_filters,\n",
    "                                kernel_size = 4,\n",
    "                                padding = 'valid',\n",
    "                                activation = 'relu')\n",
    "\n",
    "    self.pool = layers.GlobalMaxPool1D() #Camada de pooling\n",
    "\n",
    "    self.dense_1 = layers.Dense(units = FFN_units, activation = 'relu') #Camada mais densa\n",
    "\n",
    "    self.dropout = layers.Dropout(rate = dropout_rate) #Camada de dropout\n",
    "\n",
    "    if nb_classes == 2:\n",
    "      self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')\n",
    "    else:\n",
    "      self.leat_dense = layers.Dense(units = nb_classes, activation = 'softmax')\n",
    "\n",
    "  def embed_with_bert(self, all_tokens):\n",
    "    _, emb = self.bert_layer([all_tokens[:,0,:],\n",
    "                              all_tokens[:,1,:],\n",
    "                              all_tokens[:,2,:]])\n",
    "    return emb\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    x = self.embed_with_bert(inputs)\n",
    "    x_1 = self.bigram(x)\n",
    "    x_1 = self.pool(x_1)\n",
    "    x_2 = self.trigram(x)\n",
    "    x_2 = self.pool(x_2)\n",
    "    x_3 = self.fourgram(x)\n",
    "    x_3 = self.pool(x_3)\n",
    "\n",
    "    merged = tf.concat([x_1, x_2, x_3], axis = -1)\n",
    "    merged = self.dense_1(merged)\n",
    "    merged = self.dropout(merged, training)\n",
    "    output = self.last_dense(merged)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 7: Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256 #Número de neuronios na camada densa\n",
    "NB_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2 #Zerar neuronios\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn = DCNNBERTEmbedding(nb_filters = NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes = NB_CLASSES,\n",
    "            dropout_rate = DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "  Dcnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "else:\n",
    "  Dcnn.compile(loss = 'sparce_categorical_crossentropy', optimizer = 'adam', metrics = ['sparce_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/content/drive/MyDrive/embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cktp = tf.train.Checkpoint(Dcnn=Dcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cktp_manager = tf.train.CheckpointManager(cktp, checkpoint_path, max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cktp_manager.latest_checkpoint:\n",
    "  cktp.restore(cktp_manager.latest_checkpoint)\n",
    "  print('Ultimo checkpoint resturado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallBack(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    cktp_manager.save()\n",
    "    print(\"Checkpoint salvo!\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Dcnn.fit(train_dataset,\n",
    "                   epochs = NB_EPOCHS,\n",
    "                   steps_per_epoch = 100,\n",
    "                   callbacks=[MyCustomCallBack()]) #Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 8: Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Accuracy progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\n",
    "  tokens = encode_sentence(sentence)\n",
    "\n",
    "  input_ids = get_ids(tokens)\n",
    "  input_mask = get_mask(tokens)\n",
    "  segment_ids = get_segments(tokens)\n",
    "\n",
    "  inputs = tf.stack(\n",
    "     [\n",
    "      tf.cast(input_ids, dtype=tf.int32),\n",
    "      tf.cast(input_mask, dtype=tf.int32),\n",
    "      tf.cast(segment_ids, dtype=tf.int32),\n",
    "     ], axis=0)\n",
    "  inputs = tf.expand_dims(inputs, 0)\n",
    "  output = Dcnn(inputs, training=False)\n",
    "  sentiment = math.floor(output*2)\n",
    "\n",
    "  if sentiment == 0:\n",
    "    print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(output))\n",
    "  if sentiment == 1:\n",
    "    print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction(\"I beautiful girl!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
